# -*- coding: utf-8 -*-
"""7_19101100_MarshiaNujhat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y7dxNPQRIj1vS3dGulQOROaTuuvbCig1

**Lab_Assignment4_19101100**

**Loading dataset using pandas**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

leaf_data= pd.read_csv('/content/sample_data/leaf_dataset.csv')

print(leaf_data.shape)
leaf_data

leaf_data.keys()

"""**Replacing null values**"""

leaf_data.isnull().sum()

#need to replace null values with mean of each column from Elongation, Maximal Indentation Depth, Lobedness and Average Contrast

from sklearn.impute import SimpleImputer

impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(leaf_data[['Elongation']])
leaf_data['Elongation']=impute.transform(leaf_data[['Elongation']])

impute.fit(leaf_data[['Maximal Indentation Depth']])
leaf_data['Maximal Indentation Depth']=impute.transform(leaf_data[['Maximal Indentation Depth']])

impute.fit(leaf_data[['Lobedness']])
leaf_data['Lobedness']=impute.transform(leaf_data[['Lobedness']])

impute.fit(leaf_data[['Average Contrast']])
leaf_data['Average Contrast']=impute.transform(leaf_data[['Average Contrast']])

# leaf_data[['Elongation']].head(20)
leaf_data.head(10)

#There are no categorical values in my dataset.

"""**Splitting dataset into features and labels**"""

from sklearn.model_selection import train_test_split

# features=np.array(leaf_data.iloc[:,1:16])
# labels=np.array(leaf_data.iloc[:,:1])
features=leaf_data[['specimen number','Eccentricity','Aspect Ratio','Elongation','Solidity','Stochastic Convexity','Isoperimetric Factor','Maximal Indentation Depth','Lobedness','Average Intensity','Average Contrast','Smoothness','Third moment','Uniformity','Entropy']]
labels=leaf_data[['Class(species)']]
X_train, X_test, Y_train, Y_test=train_test_split(features,labels,test_size=0.25)
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

X_train

X_test

Y_train

Y_test

"""**MinMaxScaling**"""

print("Maximum value of each feature before scaling")
for i in leaf_data.keys():
  print(i,':',max(leaf_data[i]))

#We see that many features like specimen number,Entropy have a larger range while many are smaller so equal distribution is needed by scaling.

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

pd.DataFrame(X_train_scaled)

pd.DataFrame(X_test_scaled)

print("Maximum value of each feature after scaling:\n {}".format(X_train_scaled.max(axis=0)))
print("Minimum value of each feature after scaling:\n {}".format(X_train_scaled.min(axis=0)))